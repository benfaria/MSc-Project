# MSc-Project
Code repository for MSc Project on bias measurement and mitigation in ML models for consumers
Abstract

Measuring and mitigating bias in machine learning classification algorithms is relatively a new field in data science. There are numerous examples of bias in classification algorithms. There are no current examples that have been successfully challenged under discrimination law. Nevertheless, it is becoming imperative that this bias is addressed and mitigated against. Left untreated, bias in classification datasets can expose businesses to legal risk, and limit opportunity. 

Datasets used to train classification models are the main source of bias. They often comprise data and feature selections that reflect historic social and economic disparities. Classifiers work by finding statistical relationships and patterns between attributes in the training dataset. Bias emerges when a sensitive attribute such as gender or race exhibits an overwhelming influence of the classifierâ€™s outcome, usually unintended. Mitigation techniques attempt to minimise this influence without compromising model performance. 

This assessment reviewed the various approaches in mitigating bias in datasets. Bias was measured in terms of achieving fairness for individuals and groups. It used selected metrics and mitigation algorithms from the AIF360 open-source toolkit to measure and remove bias from three public datasets. Baseline measures of bias were compared with values from the post-mitigation process to assess the effectiveness of each mitigation algorithm. The results were discussed in terms of maximising bias mitigation and classifier performance. This report also discussed the business impacts of debiasing classification algorithms.

The entire write up of the project can be found in MSc_Thesis_Report.pdf in this repository.
