{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data handling/display libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from typing import List, Union, Dict\n",
    "# put this lower down.....\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Import scikit-learn core slibraries\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import IBM's AI Fairness tooolbox\n",
    "from aif360.datasets import BinaryLabelDataset  # To handle the data\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric  # For calculating metrics\n",
    "from aif360.explainers import MetricTextExplainer  # For explaining metrics\n",
    "from aif360.algorithms.preprocessing import Reweighing  # Preprocessing technique\n",
    "\n",
    "# Warnings will be used to silence various model warnings for tidier output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>LIMIT_BAL_invalid_flag</th>\n",
       "      <th>SEX</th>\n",
       "      <th>SEX_invalid_flag</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>EDUCATION_invalid_flag</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>MARRIAGE_invalid_flag</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGE_invalid_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT3_invalid_flag</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT4_invalid_flag</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT5_invalid_flag</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>PAY_AMT6_invalid_flag</th>\n",
       "      <th>DEFAULT</th>\n",
       "      <th>DEFAULT_invalid_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  LIMIT_BAL_invalid_flag  SEX  SEX_invalid_flag  EDUCATION  \\\n",
       "0    20000.0                       0  2.0                 0        2.0   \n",
       "1   120000.0                       0  2.0                 0        2.0   \n",
       "2    90000.0                       0  2.0                 0        2.0   \n",
       "3    50000.0                       0  2.0                 0        2.0   \n",
       "4    50000.0                       0  1.0                 0        2.0   \n",
       "\n",
       "   EDUCATION_invalid_flag  MARRIAGE  MARRIAGE_invalid_flag   AGE  \\\n",
       "0                       0       1.0                      0  24.0   \n",
       "1                       0       2.0                      0  26.0   \n",
       "2                       0       2.0                      0  34.0   \n",
       "3                       0       1.0                      0  37.0   \n",
       "4                       0       1.0                      0  57.0   \n",
       "\n",
       "   AGE_invalid_flag          ...           PAY_AMT3  PAY_AMT3_invalid_flag  \\\n",
       "0                 0          ...                0.0                      0   \n",
       "1                 0          ...             1000.0                      0   \n",
       "2                 0          ...             1000.0                      0   \n",
       "3                 0          ...             1200.0                      0   \n",
       "4                 0          ...            10000.0                      0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT4_invalid_flag  PAY_AMT5  PAY_AMT5_invalid_flag  PAY_AMT6  \\\n",
       "0       0.0                      0       0.0                      0       0.0   \n",
       "1    1000.0                      0       0.0                      0    2000.0   \n",
       "2    1000.0                      0    1000.0                      0    5000.0   \n",
       "3    1100.0                      0    1069.0                      0    1000.0   \n",
       "4    9000.0                      0     689.0                      0     679.0   \n",
       "\n",
       "   PAY_AMT6_invalid_flag  DEFAULT  DEFAULT_invalid_flag  \n",
       "0                      0      1.0                     0  \n",
       "1                      0      1.0                     0  \n",
       "2                      0      0.0                     0  \n",
       "3                      0      0.0                     0  \n",
       "4                      0      0.0                     0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./input/Taiwan-Credit-Card-New.csv')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is y_train Counter () Counter({0.0: 2336, 1.0: 664})\n",
      "this is y_test Counter () Counter({0.0: 21028, 1.0: 5972})\n",
      "this is y_train unique [0. 1.]\n",
      "this is y_test  unique [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "X = train.drop(\"DEFAULT\",axis=1)\n",
    "y = train[\"DEFAULT\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.9, random_state = 101, stratify=y)\n",
    "\n",
    "from collections import Counter\n",
    "print ('this is y_train Counter ()', Counter(y_train))\n",
    "print('this is y_test Counter ()', Counter(y_test))\n",
    "print('this is y_train unique', np.unique(y_train))\n",
    "print('this is y_test  unique', np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression validation accuracy: 0.7784074074074074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      1.00      0.88     21028\n",
      "         1.0       0.26      0.00      0.00      5972\n",
      "\n",
      "    accuracy                           0.78     27000\n",
      "   macro avg       0.52      0.50      0.44     27000\n",
      "weighted avg       0.66      0.78      0.68     27000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[21011,    17],\n",
       "       [ 5966,     6]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biasedlogmodel = LogisticRegression(random_state=101)\n",
    "biasedlogmodel.fit(X_train,y_train)\n",
    "logpredictions = biasedlogmodel.predict(X_test)\n",
    "print(f\"Logistic regression validation accuracy: {biasedlogmodel.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, logpredictions))\n",
    "confusion_matrix(y_test, logpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest validation accuracy: 0.8119259259259259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.97      0.89     21028\n",
      "         1.0       0.69      0.27      0.39      5972\n",
      "\n",
      "    accuracy                           0.81     27000\n",
      "   macro avg       0.76      0.62      0.64     27000\n",
      "weighted avg       0.79      0.81      0.78     27000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[20298,   730],\n",
       "       [ 4348,  1624]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biasedrfcmodel = RandomForestClassifier(n_estimators=100,max_depth=5)\n",
    "biasedrfcmodel.fit(X_train, y_train)\n",
    "rfcpredictions = biasedrfcmodel.predict(X_test)\n",
    "print(f\"Random forest validation accuracy: {biasedrfcmodel.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, rfcpredictions))\n",
    "confusion_matrix(y_test, rfcpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Binary Label Dataset to use with AIF360 APIs\n",
    "train_pp_bld = BinaryLabelDataset(df=pd.concat((X_train, y_train),\n",
    "                                               axis=1),\n",
    "                                  label_names=['DEFAULT'],\n",
    "                                  protected_attribute_names=['SEX'],\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "privileged_groups = [{'SEX': 1}]\n",
    "unprivileged_groups = [{'SEX': 2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAdditions:\n",
    "    def explain(self,\n",
    "                disp: bool=True) -> Union[None, str]:\n",
    "        \"\"\"Explain everything available for the given metric.\"\"\"\n",
    "\n",
    "        # Find intersecting methods/attributes between MetricTextExplainer and provided metric.\n",
    "        inter = set(dir(self)).intersection(set(dir(self.metric)))\n",
    "\n",
    "        # Ignore private and dunder methods\n",
    "        metric_methods = [getattr(self, c) for c in inter if c.startswith('_') < 1]\n",
    "\n",
    "        # Call methods, join to new lines\n",
    "        s = \"\\n\".join([f() for f in metric_methods if callable(f)])\n",
    "\n",
    "        if disp:\n",
    "            print(s)\n",
    "        else:\n",
    "            return s  \n",
    "        \n",
    "class MetricTextExplainer_(MetricTextExplainer, MetricAdditions):\n",
    "    \"\"\"Combine explainer and .explain.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of instances  : 3000.0\n",
      "Base Rate       : 0.7786666666666666\n",
      "Consistency     : [0.74186667]\n",
      "Disparate Impact: 1.0333369537670614\n",
      "Mean Difference : 0.025454324304899023\n",
      "# of negatives(privileged)    : 288.0\n",
      "# of negatives(non-privileged): 376.0\n",
      "# of positives(privileged)    : 930.0\n",
      "# of positives(non-privileged): 1406.0\n",
      "Statistical Parity Diference  : 0.025454324304899023\n",
      "\n",
      "This is the explainer\n",
      "Mean difference (mean label value on privileged instances - mean label value on unprivileged instances): 0.025454324304899023\n",
      "Number of instances: 3000.0\n",
      "Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 1.0333369537670614\n",
      "Number of negative-outcome instances: 664.0\n",
      "Statistical parity difference (probability of favorable outcome for unprivileged instances - probability of favorable outcome for privileged instances): 0.025454324304899023\n",
      "Number of positive-outcome instances: 2336.0\n",
      "Consistency (Zemel, et al. 2013): [0.74186667]\n"
     ]
    }
   ],
   "source": [
    "# Create the metric object from the Binary Label Dataset.\n",
    "metric_train_bld = BinaryLabelDatasetMetric(train_pp_bld,\n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)\n",
    "print(\"# of instances  :\", metric_train_bld.num_instances())\n",
    "print(\"Base Rate       :\", metric_train_bld.base_rate())\n",
    "print(\"Consistency     :\", metric_train_bld.consistency())\n",
    "print(\"Disparate Impact:\", metric_train_bld.disparate_impact())\n",
    "print(\"Mean Difference :\", metric_train_bld.mean_difference())\n",
    "print(\"# of negatives(privileged)    :\", metric_train_bld.num_negatives(privileged=True))\n",
    "print(\"# of negatives(non-privileged):\", metric_train_bld.num_negatives(privileged=False))\n",
    "print(\"# of positives(privileged)    :\", metric_train_bld.num_positives(privileged=True))\n",
    "print(\"# of positives(non-privileged):\", metric_train_bld.num_positives(privileged=False))\n",
    "print(\"Statistical Parity Diference  :\", metric_train_bld.statistical_parity_difference()) \n",
    "print(\"\")\n",
    "print(\"This is the explainer\")\n",
    "# Create the explainer object\n",
    "explainer = MetricTextExplainer_(metric_train_bld)\n",
    "# Explain relevant metrics\n",
    "explainer.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now attempt to mitigate the bias using pre-processing Reweighing, apply to Training dataset\n",
    "rw = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "# trained model with new weights..\n",
    "train_pp_bld_f = rw.fit_transform(train_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe from Training dataset and new weights and have a look! \n",
    "pd.DataFrame({'SEX': X_train.SEX,\n",
    "              'DEFAULT': y_train,\n",
    "              'Original_weight': np.ones(shape=(X_train.shape[0],)),\n",
    "              'new_weight': train_pp_bld_f.instance_weights}).sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new Dataframe \n",
    "unbiasedlogmodel = LogisticRegression()\n",
    "unbiasedlogmodel.fit(X_train, y_train, sample_weight=train_pp_bld_f.instance_weights)\n",
    "print(f\"Biased   Logistic regression validation accuracy: {biasedlogmodel.score(X_test,y_test)}\")\n",
    "print(f\"Unbiased Logistic regression validation accuracy: {unbiasedlogmodel.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiasedrfcmodel = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "unbiasedrfcmodel.fit(X_train, y_train, sample_weight=train_pp_bld_f.instance_weights)\n",
    "print(f\"Biased   Random forest validation accuracy: {biasedrfcmodel.score(X_test, y_test)}\")\n",
    "print(f\"Unbiased Random forest validation accuracy: {unbiasedrfcmodel.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(y_true: np.ndarray, preds: Dict[str, np.ndarray],\n",
    "             title: str='', \n",
    "             ax=None) -> None:\n",
    "    leg = []\n",
    "    for k, p in preds.items():\n",
    "        fpr, tpr, _ = roc_curve(y_true, p)\n",
    "        ax = sns.lineplot(x=fpr, \n",
    "                          y=tpr,\n",
    "                          ci=None,\n",
    "                          ax=ax\n",
    "                         )\n",
    "        leg.append(f\"{k}: {round(auc(fpr, tpr), 2)}\")\n",
    "   \n",
    "    ax.legend(leg)\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_ylabel('TPR')\n",
    "    sns.lineplot(x=[0, 1],\n",
    "                 y=[0, 1],\n",
    "                 color='gray',\n",
    "                 ax=ax)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    \n",
    "print('Accuracy:')\n",
    "display(pd.DataFrame({'LogReg': [biasedlogmodel.score(X_test, y_test), \n",
    "                                 unbiasedlogmodel.score(X_test, y_test)],\n",
    "                      'RFC': [biasedrfcmodel.score(X_test, y_test),\n",
    "                              unbiasedrfcmodel.score(X_test, y_test)]}, \n",
    "                     index =['Unfair', 'Fair']))\n",
    "\n",
    "print('AUC:')\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "plot_auc(y_test, \n",
    "         {'biased': biasedlogmodel.predict_proba(X_test)[:, 1],\n",
    "          'unbiased': unbiasedlogmodel.predict_proba(X_test)[:, 1]},\n",
    "         title='LR',\n",
    "         ax=ax[0]) \n",
    "plot_auc(y_test, \n",
    "         {'biased': biasedrfcmodel.predict_proba(X_test)[:, 1],\n",
    "          'unbiased': unbiasedrfcmodel.predict_proba(X_test)[:, 1]},\n",
    "         title='RFC',\n",
    "         ax=ax[1]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(mod: Union[LogisticRegression, RandomForestClassifier],\n",
    "                       names: List[str],\n",
    "                       scale=None) -> pd.DataFrame:\n",
    "    \"\"\"Return feature importance for LR or RFC models in a sorted DataFrame.\"\"\"\n",
    "    if type(mod) == LogisticRegression:\n",
    "        imp = np.abs(mod.coef_.squeeze()) / scale\n",
    "        var = np.zeros(shape=imp.shape)\n",
    "    elif type(mod) == RandomForestClassifier:\n",
    "        imps = np.array([fi.feature_importances_ for fi in mod.estimators_])\n",
    "        imp = imps.mean(axis=0)\n",
    "        var = imps.std(axis=0)\n",
    "\n",
    "    return pd.DataFrame({'feature': names,\n",
    "                         'importance': imp,\n",
    "                         'std': var}).sort_values('importance',\n",
    "                                                  ascending=False)\n",
    "\n",
    "def plot_feature_importance(**kwargs) -> None:\n",
    "    ax = sns.barplot(**kwargs)\n",
    "    for l in ax.get_xticklabels():\n",
    "        l.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(biasedlogmodel,\n",
    "                                                names=X_train.columns.tolist(),\n",
    "                                                scale=X_train.std()),\n",
    "                       ax=ax[1])\n",
    "_ = ax[0].set_title('LR Feature Importance - before Mitigating')\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(biasedrfcmodel,\n",
    "                                                names=X_train.columns.tolist()),\n",
    "                       ax=ax[0])\n",
    "_ = ax[1].set_title('RFC Feature Importance - before Mitigating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(unbiasedlogmodel,\n",
    "                                                names=X_train.columns.tolist(),\n",
    "                                                scale=X_train.std()),\n",
    "                       ax=ax[1])\n",
    "_ = ax[0].set_title('LR Feature Importance - after Mitigating')\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(unbiasedrfcmodel,\n",
    "                                                names=X_train.columns.tolist()),\n",
    "                       ax=ax[0])\n",
    "_ = ax[1].set_title('RFC Feature Importance - after Mitigating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(mod, x: pd.DataFrame, y_true: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate fairness metrics at each model threshold.\"\"\"\n",
    "    \n",
    "    # Create a BinaryLabelDataset (as before training)\n",
    "    bld = BinaryLabelDataset(df=pd.concat((x, y_true),\n",
    "                                               axis=1),\n",
    "                                  label_names=['DEFAULT'],\n",
    "                                  protected_attribute_names=['SEX'],\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "    privileged_groups = [{'SEX': 1}]\n",
    "    unprivileged_groups = [{'SEX': 2}]\n",
    "    \n",
    "    # Create a second set to hold the predicted labels\n",
    "    bld_preds = bld.copy(deepcopy=True)\n",
    "    preds = mod.predict_proba(x)[:, 1] # for all the rows, retain the 2nd value.\n",
    "                                       # preds will contain a single column table of all \n",
    "                                       # probabilities that the classification will be 1.\n",
    "    accuracy = []\n",
    "    balanced_accuracy = []\n",
    "    disp_impact = []\n",
    "    average_abs_odds_difference = []\n",
    "    avg_odd_diff = []\n",
    "    equal_opportunity_difference = []\n",
    "    error_rate = []\n",
    "    \n",
    "    \n",
    "    # For threshold values between 0 and 1:\n",
    "    thresh = np.linspace(0.01, 0.99, 100) # generate 100 evenly spaced values from 0.01->0.99 \n",
    "    for t in thresh:\n",
    "        \n",
    "        # Apply threshold and set labels in bld for predictions\n",
    "        bld_preds.labels[preds > t] = 1  # labels is a structured dataset attribute.\n",
    "        bld_preds.labels[preds <= t] = 0\n",
    "\n",
    "        # Calculate the metrics for this threshold\n",
    "        valid_metric = ClassificationMetric(bld, bld_preds, \n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)\n",
    "\n",
    "        # Save the balanced accuracy of the model, and the metrics\n",
    "        accuracy.append(valid_metric.accuracy())\n",
    "        balanced_accuracy.append(0.5 * (valid_metric.true_positive_rate()\n",
    "                                        + valid_metric.true_negative_rate()))\n",
    "        disp_impact.append(np.abs(valid_metric.disparate_impact() - 0.5))\n",
    "        average_abs_odds_difference.append(valid_metric.average_abs_odds_difference())\n",
    "        avg_odd_diff.append(valid_metric.average_odds_difference())\n",
    "        equal_opportunity_difference.append(valid_metric.equal_opportunity_difference())\n",
    "        error_rate.append(valid_metric.error_rate())\n",
    "\n",
    "    # Return as df indexed by threshold\n",
    "    metrics = pd.DataFrame({'accuracy': accuracy,\n",
    "                            'balanced_accuracy': balanced_accuracy,\n",
    "                            'disparate_impact': disp_impact,\n",
    "                            'average_abs_odds_difference': average_abs_odds_difference,\n",
    "                            'avg_odds_diff': avg_odd_diff,\n",
    "                            'equal_opportunity_diff': equal_opportunity_difference},\n",
    "                            index=thresh)\n",
    "#'error_rate': error_rate},\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_metrics(metrics: pd.DataFrame, \n",
    "                 title: str='', **kwargs) -> None:\n",
    "    \"\"\"Plot the metrics df from calc_metrics with seaborn.\"\"\"\n",
    "    ax = sns.lineplot(data=metrics, \n",
    "                      **kwargs)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Classification threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for LR\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', RuntimeWarning)\n",
    "    \n",
    "    plot_metrics(calc_metrics(biasedlogmodel, X_test, y_test),\n",
    "                ax=ax[0],\n",
    "                title=\"LR: Biased\")\n",
    "    \n",
    "    plot_metrics(calc_metrics(unbiasedlogmodel, X_test, y_test),\n",
    "                ax=ax[1],\n",
    "                title='\"LR: Fair\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for RFC\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', RuntimeWarning)\n",
    "    \n",
    "    plot_metrics(calc_metrics(biasedrfcmodel, X_test, y_test),\n",
    "                ax=ax[0],\n",
    "                title=\"RFC: Biased\")\n",
    "    \n",
    "    plot_metrics(calc_metrics(unbiasedrfcmodel, X_test, y_test),\n",
    "                ax=ax[1],\n",
    "                title='\"RFC: Fair\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
