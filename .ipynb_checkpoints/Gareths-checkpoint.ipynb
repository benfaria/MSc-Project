{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use type hinting\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "# Modelling. Warnings will be used to silence various model warnings for tidier output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "\n",
    "# Data handling/display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "\n",
    "# IBM's fairness tooolbox:\n",
    "from aif360.datasets import BinaryLabelDataset  # To handle the data\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric  # For calculating metrics\n",
    "from aif360.explainers import MetricTextExplainer  # For explaining metrics\n",
    "from aif360.algorithms.preprocessing import Reweighing  # Preprocessing technique\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "test.loc[:, 'Survived'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing will be done using a sklearn pipeline. We need these bits to make the transformers and connect them.\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# For the logistic regression model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectCols(TransformerMixin):\n",
    "    \"\"\"Select columns from a DataFrame.\"\"\"\n",
    "    def __init__(self, cols: List[str]) -> None:\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, x: None) -> \"SelectCols\":\n",
    "        \"\"\"Nothing to do.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Return just selected columns.\"\"\"\n",
    "        return x[self.cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SelectCols(cols=['Sex', 'Survived','Name'])\n",
    "sc.transform(train.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(TransformerMixin):\n",
    "    \"\"\"Convert non-numeric columns to numeric using label encoding. \n",
    "    Handles unseen data on transform.\"\"\"\n",
    "    def fit(self, x: pd.DataFrame) -> \"LabelEncoder\":\n",
    "        \"\"\"Learn encoder for each column.\"\"\"\n",
    "        encoders = {}\n",
    "        for c in x:\n",
    "            # Make encoder using pd.factorize on unique values, \n",
    "            # then convert to a dictionary\n",
    "            v, k = zip(pd.factorize(x[c].unique()))\n",
    "            encoders[c] = dict(zip(k[0], v[0]))\n",
    "\n",
    "        self.encoders_ = encoders\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x) -> pd.DataFrame:\n",
    "        \"\"\"For columns in x that have learned encoders, apply encoding.\"\"\"\n",
    "        x = x.copy()\n",
    "        for c in x:\n",
    "            # Ignore new, unseen values\n",
    "            x.loc[~x[c].isin(self.encoders_[c]), c] = np.nan\n",
    "            # Map learned labels\n",
    "            x.loc[:, c] = x[c].map(self.encoders_[c])\n",
    "\n",
    "        # Return without nans\n",
    "        return x.fillna(-2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit_transform(train[['Pclass', 'Sex']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.encoders_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericEncoder(TransformerMixin):\n",
    "    \"\"\"Remove invalid values from numerical columns, replace with median.\"\"\"\n",
    "    def fit(self, x: pd.DataFrame) -> \"NumericEncoder\":\n",
    "        \"\"\"Learn median for every column in x.\"\"\"\n",
    "        # Find median for all columns, handling non-NaNs invalid values and NaNs\n",
    "        # Where all values are NaNs (after coercion) the median value will be a NaN.\n",
    "        self.encoders_ = {\n",
    "            c: pd.to_numeric(x[c],\n",
    "                             errors='coerce').median(skipna=True) for c in x}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"For each column in x, encode NaN values are learned \n",
    "        median and add a flag column indicating where these \n",
    "        replacements were made\"\"\"\n",
    "\n",
    "        # Create a list of new DataFrames, each with 2 columns\n",
    "        output_dfs = []\n",
    "        for c in x:\n",
    "            new_cols = pd.DataFrame()\n",
    "            # Find invalid values that aren't nans (-inf, inf, string)\n",
    "            invalid_idx = pd.to_numeric(x[c].replace([-np.inf, np.inf],\n",
    "                                                     np.nan),\n",
    "                                        errors='coerce').isnull()\n",
    "\n",
    "            # Copy to new df for this column\n",
    "            new_cols.loc[:, c] = x[c].copy()\n",
    "            # Replace the invalid values with learned median\n",
    "            new_cols.loc[invalid_idx, c] = self.encoders_[c]\n",
    "            # Mark these replacement in a new column called \n",
    "            # \"[column_name]_invalid_flag\"\n",
    "            new_cols.loc[:, f\"{c}_invalid_flag\"] = invalid_idx.astype(np.int8)\n",
    "\n",
    "            output_dfs.append(new_cols)\n",
    "\n",
    "        # Concat list of output_dfs to single df\n",
    "        df = pd.concat(output_dfs,\n",
    "                       axis=1)\n",
    "\n",
    "        # Return wtih an remaining NaNs removed. These might exist if the median\n",
    "        # is a NaN because there was no numeric data in the column at all.\n",
    "        return df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne = NumericEncoder()\n",
    "ne.fit_transform(train[['Age', 'Fare']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne.encoders_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constructing the pipeline\n",
    "\n",
    "# LabelEncoding fork: Select object columns -> label encode\n",
    "pp_object_cols = Pipeline([('select', SelectCols(cols=['Sex', 'Survived', \n",
    "                                                       'Cabin', 'Ticket', \n",
    "                                                       'SibSp', 'Embarked',\n",
    "                                                       'Parch', 'Pclass',\n",
    "                                                       'Name'])),\n",
    "                           ('process', LabelEncoder())])\n",
    "\n",
    "# NumericEncoding fork: Select numeric columns -> numeric encode\n",
    "pp_numeric_cols = Pipeline([('select', SelectCols(cols=['Age', \n",
    "                                                        'Fare'])),\n",
    "                            ('process', NumericEncoder())])\n",
    "\n",
    "\n",
    "# We won't use the next part, but typically the pipeline would continue to \n",
    "# the model (after dropping 'Survived' from the training data, of course). \n",
    "# For example:\n",
    "pp_pipeline = FeatureUnion([('object_cols', pp_object_cols),\n",
    "                            ('numeric_cols', pp_numeric_cols)])\n",
    "\n",
    "model_pipeline = Pipeline([('pp', pp_pipeline),\n",
    "                           ('mod', LogisticRegression())])\n",
    "# This could be run with model.pipeline.fit_predict(x), and passed to a \n",
    "# gridsearch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, valid = train_test_split(train,\n",
    "                                 test_size=0.3)\n",
    "\n",
    "# .fit_transform on train\n",
    "train_pp = pd.concat((pp_numeric_cols.fit_transform(train_), \n",
    "                      pp_object_cols.fit_transform(train_)),\n",
    "                     axis=1)\n",
    "print(train_pp.Age.count())\n",
    "\n",
    "\n",
    "# .transform on valid\n",
    "valid_pp = pd.concat((pp_numeric_cols.transform(valid), \n",
    "                      pp_object_cols.transform(valid)),\n",
    "                     axis=1)\n",
    "print(valid_pp.Age.count())\n",
    "#valid_pp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .transform on test\n",
    "test_pp = pd.concat((pp_numeric_cols.transform(test), \n",
    "                     pp_object_cols.transform(test)),\n",
    "                    axis=1)\n",
    "print(test_pp.Age.count())\n",
    "#test_pp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Survived'\n",
    "x_columns = [c for c in train_pp if c != target]\n",
    "x_train, y_train = train_pp[x_columns], train_pp[target]\n",
    "x_valid, y_valid = valid_pp[x_columns], valid_pp[target]\n",
    "x_test = test_pp[x_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_lr = LogisticRegression()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "    biased_lr.fit(x_train, y_train)\n",
    "    \n",
    "print(f\"Logistic regression validation accuracy: {biased_lr.score(x_valid, y_valid)}\")\n",
    "\n",
    "sub.loc[:, 'Survived'] = biased_lr.predict(x_test).astype(int)\n",
    "sub.to_csv('biased_lr.csv', \n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_rfc = RandomForestClassifier(n_estimators=100, \n",
    "                                    max_depth=4)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "    \n",
    "    biased_rfc.fit(x_train, y_train)\n",
    "    \n",
    "print(f\"Random forest validation accuracy: {biased_rfc.score(x_valid, y_valid)}\")\n",
    "\n",
    "sub.loc[:, 'Survived'] = biased_rfc.predict(x_test).astype(int)\n",
    "sub.to_csv('biased_rfc.csv', \n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pp_bld = BinaryLabelDataset(df=pd.concat((x_train, y_train),\n",
    "                                               axis=1),\n",
    "                                  label_names=['Survived'],\n",
    "                                  protected_attribute_names=['Sex'],\n",
    "                                  favorable_label=1,\n",
    "                                  unfavorable_label=0)\n",
    "\n",
    "privileged_groups = [{'Sex': 1}]\n",
    "unprivileged_groups = [{'Sex': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAdditions:\n",
    "    def explain(self,\n",
    "                disp: bool=True) -> Union[None, str]:\n",
    "        \"\"\"Explain everything available for the given metric.\"\"\"\n",
    "\n",
    "        # Find intersecting methods/attributes between MetricTextExplainer and provided metric.\n",
    "        inter = set(dir(self)).intersection(set(dir(self.metric)))\n",
    "\n",
    "        # Ignore private and dunder methods\n",
    "        metric_methods = [getattr(self, c) for c in inter if c.startswith('_') < 1]\n",
    "\n",
    "        # Call methods, join to new lines\n",
    "        s = \"\\n\".join([f() for f in metric_methods if callable(f)])\n",
    "\n",
    "        if disp:\n",
    "            print(s)\n",
    "        else:\n",
    "            return s\n",
    "        \n",
    "        \n",
    "class MetricTextExplainer_(MetricTextExplainer, MetricAdditions):\n",
    "    \"\"\"Combine explainer and .explain.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the metric object\n",
    "metric_train_bld = BinaryLabelDatasetMetric(train_pp_bld,\n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)\n",
    "\n",
    "# Create the explainer object\n",
    "explainer = MetricTextExplainer_(metric_train_bld)\n",
    "# Explain relevant metrics\n",
    "explainer.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "train_pp_bld_f = rw.fit_transform(train_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Sex': x_train.Sex,\n",
    "              'Survived': y_train,\n",
    "              'Original_weight': np.ones(shape=(x_train.shape[0],)),\n",
    "              'new_weight': train_pp_bld_f.instance_weights}).sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_lr = LogisticRegression()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "        \n",
    "    unbiased_lr.fit(x_train, y_train,\n",
    "                    sample_weight=train_pp_bld_f.instance_weights)\n",
    "    \n",
    "print(f\"Logistic regression validation accuracy: {unbiased_lr.score(x_valid, y_valid)}\")\n",
    "\n",
    "sub.loc[:, 'Survived'] = unbiased_lr.predict(x_test).astype(int)\n",
    "sub.to_csv('unbiased_lr.csv', \n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_rfc = RandomForestClassifier(n_estimators=100,\n",
    "                                      max_depth=4)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "    \n",
    "    unbiased_rfc.fit(x_train, y_train,\n",
    "                     sample_weight=train_pp_bld_f.instance_weights)\n",
    "    \n",
    "print(f\"Random forest validation accuracy: {unbiased_rfc.score(x_valid, y_valid)}\")\n",
    "\n",
    "sub.loc[:, 'Survived'] = unbiased_rfc.predict(x_test).astype(int)\n",
    "sub.to_csv('unbiased_rfc.csv', \n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(y_true: np.ndarray, preds: Dict[str, np.ndarray],\n",
    "             title: str='', \n",
    "             ax=None) -> None:\n",
    "    \n",
    "    leg = []\n",
    "    for k, p in preds.items():\n",
    "        fpr, tpr, _ = roc_curve(y_true, p)\n",
    "        ax = sns.lineplot(x=fpr, \n",
    "                          y=tpr,\n",
    "                          ci=None,\n",
    "                          ax=ax\n",
    "                         )\n",
    "        leg.append(f\"{k}: {round(auc(fpr, tpr), 2)}\")\n",
    "    \n",
    "    ax.legend(leg)\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_ylabel('TPR')\n",
    "    sns.lineplot(x=[0, 1],\n",
    "                 y=[0, 1],\n",
    "                 color='gray',\n",
    "                 ax=ax)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    \n",
    "print('Accuracy:')\n",
    "display(pd.DataFrame({'LogReg': [biased_lr.score(x_valid, y_valid), \n",
    "                                 unbiased_lr.score(x_valid, y_valid)],\n",
    "                      'RFC': [biased_rfc.score(x_valid, y_valid),\n",
    "                              unbiased_rfc.score(x_valid, y_valid)]}, \n",
    "                     index =['Unfair', 'Fair']))\n",
    "\n",
    "print('AUC:')\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "plot_auc(y_valid, \n",
    "         {'biased': biased_lr.predict_proba(x_valid)[:, 1],\n",
    "          'unbiased': unbiased_lr.predict_proba(x_valid)[:, 1]},\n",
    "         title='LR',\n",
    "         ax=ax[0]) \n",
    "plot_auc(y_valid, \n",
    "         {'biased': biased_rfc.predict_proba(x_valid)[:, 1],\n",
    "          'unbiased': unbiased_rfc.predict_proba(x_valid)[:, 1]},\n",
    "         title='RFC',\n",
    "         ax=ax[1]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(mod: Union[LogisticRegression, RandomForestClassifier],\n",
    "                       names: List[str],\n",
    "                       scale=None) -> pd.DataFrame:\n",
    "    \"\"\"Return feature importance for LR or RFC models in a sorted DataFrame.\"\"\"\n",
    "    if type(mod) == LogisticRegression:\n",
    "        imp = np.abs(mod.coef_.squeeze()) / scale\n",
    "        var = np.zeros(shape=imp.shape)\n",
    "    elif type(mod) == RandomForestClassifier:\n",
    "        imps = np.array([fi.feature_importances_ for fi in mod.estimators_])\n",
    "        imp = imps.mean(axis=0)\n",
    "        var = imps.std(axis=0)\n",
    "\n",
    "    return pd.DataFrame({'feature': names,\n",
    "                         'importance': imp,\n",
    "                         'std': var}).sort_values('importance',\n",
    "                                                  ascending=False)\n",
    "\n",
    "def plot_feature_importance(**kwargs) -> None:\n",
    "    ax = sns.barplot(**kwargs)\n",
    "    for l in ax.get_xticklabels():\n",
    "        l.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(biased_lr,\n",
    "                                                names=x_train.columns.tolist(),\n",
    "                                                scale=x_train.std()),\n",
    "                       ax=ax[1])\n",
    "_ = ax[0].set_title('LR')\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(biased_rfc,\n",
    "                                                names=x_train.columns.tolist()),\n",
    "                       ax=ax[0])\n",
    "_ = ax[1].set_title('RFC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(unbiased_lr,\n",
    "                                                names=x_train.columns.tolist(),\n",
    "                                                scale=x_train.std()),\n",
    "                       ax=ax[1])\n",
    "_ = ax[0].set_title('LR')\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(unbiased_rfc,\n",
    "                                                names=x_train.columns.tolist()),\n",
    "                       ax=ax[0])\n",
    "_ = ax[1].set_title('RFC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(mod, x: pd.DataFrame, y_true: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate fairness metrics at each model threshold.\"\"\"\n",
    "    \n",
    "    # Create a BinaryLabelDataset (as before training)\n",
    "    bld = BinaryLabelDataset(df=pd.concat((x, y_true),\n",
    "                                               axis=1),\n",
    "                                  label_names=['Survived'],\n",
    "                                  protected_attribute_names=['Sex'],\n",
    "                                  favorable_label=1,\n",
    "                                  unfavorable_label=0)\n",
    "\n",
    "    privileged_groups = [{'Sex': 1}]\n",
    "    unprivileged_groups = [{'Sex': 0}]\n",
    "    \n",
    "    # Create a second set to hold the predicted labels\n",
    "    bld_preds = bld.copy(deepcopy=True)\n",
    "    preds = mod.predict_proba(x)[:, 1]\n",
    "\n",
    "    balanced_accuracy = []\n",
    "    disp_impact = []\n",
    "    avg_odd_diff = []\n",
    "    \n",
    "    # For threshold values between 0 and 1:\n",
    "    thresh = np.linspace(0.01, 0.99, 100)\n",
    "    for t in thresh:\n",
    "        \n",
    "        # Apply threshold and set labels in bld for predictions\n",
    "        bld_preds.labels[preds > t] = 1\n",
    "        bld_preds.labels[preds <= t] = 0\n",
    "\n",
    "        # Calculate the metrics for this threshold\n",
    "        valid_metric = ClassificationMetric(bld, bld_preds, \n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)\n",
    "\n",
    "        # Save the balanced accuracy of the model, and the metrics\n",
    "        balanced_accuracy.append(0.5 * (valid_metric.true_positive_rate()\n",
    "                                        + valid_metric.true_negative_rate()))\n",
    "        avg_odd_diff.append(valid_metric.average_odds_difference())\n",
    "        disp_impact.append(np.abs(valid_metric.disparate_impact() - 0.5))\n",
    "\n",
    "    # Return as df indexed by threshold\n",
    "    metrics = pd.DataFrame({'balanced_accuray': balanced_accuracy,\n",
    "                            'disparate_impact': disp_impact,\n",
    "                            'avg_odds_diff': avg_odd_diff},\n",
    "                          index=thresh)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_metrics(metrics: pd.DataFrame, \n",
    "                 title: str='', **kwargs) -> None:\n",
    "    \"\"\"Plot the metrics df from calc_metrics with seaborn.\"\"\"\n",
    "    ax = sns.lineplot(data=metrics, \n",
    "                      **kwargs)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Classification threshold')\n",
    "\n",
    "# Plot for LR\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', RuntimeWarning)\n",
    "    \n",
    "    plot_metrics(calc_metrics(biased_lr, x_valid, y_valid),\n",
    "                ax=ax[0],\n",
    "                title=\"LR: Biased\")\n",
    "    \n",
    "    plot_metrics(calc_metrics(unbiased_lr, x_valid, y_valid),\n",
    "                ax=ax[1],\n",
    "                title='\"Fair\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for RFC\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', RuntimeWarning)\n",
    "    \n",
    "    plot_metrics(calc_metrics(biased_rfc, x_valid, y_valid),\n",
    "                ax=ax[0],\n",
    "                title=\"RFC: Biased\")\n",
    "    \n",
    "    plot_metrics(calc_metrics(unbiased_rfc, x_valid, y_valid),\n",
    "                ax=ax[1],\n",
    "                title='\"Fair\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
