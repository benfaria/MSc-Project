{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data handling/display libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler, MinMaxScaler, RobustScaler \n",
    "from typing import List, Union, Dict\n",
    "# Warnings will be used to silence various model warnings for tidier output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Taiwan Credit Card Default training dataset\n",
    "Taiwan_df = pd.read_csv('./input/Taiwan-Credit-Card-Default.csv')\n",
    "\n",
    "# Drop the first row because it is a redundant heading row.\n",
    "Taiwan_df.to_csv('./input/Taiwan-Credit-Card-New.csv', header=False, index=False)\n",
    "\n",
    "# Re-read the dataset with its proper column headers\n",
    "Taiwan_df = pd.read_csv('./input/Taiwan-Credit-Card-New.csv')\n",
    "# Rename a couple of columns for aesthetics\n",
    "Taiwan_df.rename(columns={'SEX':'GENDER', 'PAY_0': 'PAY_1',\n",
    "                          'default payment next month':'DEFAULT'}, \n",
    "                          inplace=True)\n",
    "# Drop ID as it has no impact on this study.\n",
    "Taiwan_df.drop([\"ID\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18112\n",
       "0    11888\n",
       "Name: GENDER, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make males 0, Females 1\n",
    "gender = {1: 0,2: 1} \n",
    "Taiwan_df.GENDER = [gender[item] for item in Taiwan_df.GENDER]\n",
    "Taiwan_df.GENDER.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new column AMT_PAY_RATIO to hold the ratio of:\n",
    "# Total sum owed / Total sum paid over 6 months\n",
    "Taiwan_df['AMT_PAY_RATIO'] = (Taiwan_df['BILL_AMT1']+Taiwan_df['BILL_AMT2']+\n",
    "                              Taiwan_df['BILL_AMT3']+Taiwan_df['BILL_AMT4']+\n",
    "                              Taiwan_df['BILL_AMT5']+\n",
    "                              Taiwan_df['BILL_AMT6']) / (Taiwan_df['PAY_AMT1']+\n",
    "                                                         Taiwan_df['PAY_AMT2']+Taiwan_df['PAY_AMT3']+\n",
    "                                                         Taiwan_df['PAY_AMT4']+Taiwan_df['PAY_AMT5']+\n",
    "                                                         Taiwan_df['PAY_AMT6'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a 6 new columns called AMT-PAY-RATIO. This will contain \n",
    "# the amount/payment ratio for the 6 BILL_AMTx/PAY_AMTx. \n",
    "Taiwan_df['AMT_PAY_RATIO1'] = Taiwan_df['BILL_AMT1']/Taiwan_df['PAY_AMT1']\n",
    "Taiwan_df['AMT_PAY_RATIO2'] = Taiwan_df['BILL_AMT2']/Taiwan_df['PAY_AMT2']\n",
    "Taiwan_df['AMT_PAY_RATIO3'] = Taiwan_df['BILL_AMT3']/Taiwan_df['PAY_AMT3']\n",
    "Taiwan_df['AMT_PAY_RATIO4'] = Taiwan_df['BILL_AMT4']/Taiwan_df['PAY_AMT4']\n",
    "Taiwan_df['AMT_PAY_RATIO5'] = Taiwan_df['BILL_AMT5']/Taiwan_df['PAY_AMT5']\n",
    "Taiwan_df['AMT_PAY_RATIO6'] = Taiwan_df['BILL_AMT6']/Taiwan_df['PAY_AMT6']\n",
    "\n",
    "# We now replace the inf values (from division by zero) with NaN\n",
    "Taiwan_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# We then drop the all the original BILL_AMTx aand PAY_AMTx columns.\n",
    "Taiwan_df.drop([\"BILL_AMT1\",\"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\"], axis=1, inplace=True)\n",
    "Taiwan_df.drop([\"PAY_AMT1\", \"PAY_AMT2\" , \"PAY_AMT3\" , \"PAY_AMT4\" , \"PAY_AMT5\" , \"PAY_AMT6\" ], axis=1, inplace=True)\n",
    "#Taiwan_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do some data cleansing:\n",
    "# As the Education categories 0, 5, and 6 have no definition, \n",
    "# we shall make them 'Education Other', eg '4'\n",
    "fil = (Taiwan_df.EDUCATION == 5) | (Taiwan_df.EDUCATION == 6) | (Taiwan_df.EDUCATION == 0)\n",
    "Taiwan_df.loc[fil, 'EDUCATION'] = 4\n",
    "# Same with MARRIAGE category 0 - it has no definition, \n",
    "# so we make them 'Marriage Other', eg '3'\n",
    "Taiwan_df.loc[Taiwan_df.MARRIAGE == 0, 'MARRIAGE'] = 3\n",
    "# Taiwan_df.MARRIAGE.value_counts()\n",
    "#Taiwan_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return columns from the dataframe.\n",
    "class SelectCols(TransformerMixin):\n",
    "    \"\"\"Select columns from a DataFrame.\"\"\"\n",
    "    def __init__(self, cols: List[str]) -> None:\n",
    "        self.cols = cols\n",
    "    def fit(self, x: None) -> \"SelectCols\":\n",
    "        \"\"\"Nothing to do.\"\"\"\n",
    "        return self\n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Return just selected columns.\"\"\"\n",
    "        return x[self.cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericEncoder(TransformerMixin):\n",
    "    \"\"\"Remove invalid values from numerical columns, replace with median.\"\"\"\n",
    "    def fit(self, x: pd.DataFrame) -> \"NumericEncoder\":\n",
    "        \"\"\"Learn median for every column in x.\"\"\"\n",
    "        # Find median for all columns, handling non-NaNs invalid values and NaNs\n",
    "        # Where all values are NaNs (after coercion) the median value will be a NaN.\n",
    "        self.encoders_ = {\n",
    "            c: pd.to_numeric(x[c], errors='coerce').median(skipna=True) for c in x}\n",
    "        return self\n",
    "\n",
    "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"For each column in x, encode NaN values are learned \n",
    "        median and add a flag column indicating where these \n",
    "        replacements were made\"\"\"\n",
    "        # Create a list of new DataFrames, each with 2 columns\n",
    "        output_dfs = []\n",
    "        for c in x:\n",
    "            new_cols = pd.DataFrame()\n",
    "            # Find invalid values that aren't nans (-inf, inf, string)\n",
    "            invalid_idx = pd.to_numeric(x[c].replace([-np.inf, np.inf],np.nan),\n",
    "                                        errors='coerce').isnull()\n",
    "            # Copy to new df for this column\n",
    "            new_cols.loc[:, c] = x[c].copy()\n",
    "            # Replace the invalid values with learned median\n",
    "            new_cols.loc[invalid_idx, c] = self.encoders_[c]\n",
    "            # Mark these replacement in a new column called \n",
    "            # \"[column_name]_invalid_flag\"\n",
    "            # new_cols.loc[:, f\"{c}_invalid_flag\"] = invalid_idx.astype(np.int8)\n",
    "            output_dfs.append(new_cols)\n",
    "\n",
    "        # Concat list of output_dfs to single df\n",
    "        df = pd.concat(output_dfs,\n",
    "                       axis=1)\n",
    "        # Return wtih an remaining NaNs removed. These might exist if the median\n",
    "        # is a NaN because there was no numeric data in the column at all.\n",
    "        return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the data engineering pipeline\n",
    "# NumericEncoding fork: Select numeric columns -> numeric encode\n",
    "pp_numeric_cols = Pipeline([('select', SelectCols(cols=['LIMIT_BAL','GENDER','EDUCATION','MARRIAGE','AGE',\n",
    "                                                       'PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6',\n",
    "                                                       'AMT_PAY_RATIO1', 'AMT_PAY_RATIO2', 'AMT_PAY_RATIO3', \n",
    "                                                       'AMT_PAY_RATIO4', 'AMT_PAY_RATIO5', 'AMT_PAY_RATIO6',\n",
    "                                                       'AMT_PAY_RATIO', 'DEFAULT'])),\n",
    "                            ('process', NumericEncoder())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .fit_transform on the Taiwan dataset\n",
    "Taiwan_pp = pp_numeric_cols.fit_transform(Taiwan_df)\n",
    "# Test step - dropping columns to see whether we get meaningful results in de-biasing.\n",
    "# Taiwan_pp.drop([\"PAY_1\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"], axis=1, inplace=True)\n",
    "# Taiwan_pp.drop([\"AMT_PAY_RATIO1\",\"AMT_PAY_RATIO2\", \"AMT_PAY_RATIO3\", \"AMT_PAY_RATIO4\", \"AMT_PAY_RATIO5\", \"AMT_PAY_RATIO6\"], axis=1, inplace=True)\n",
    "# Taiwan_pp.drop([\"LIMIT_BAL\", \"AMT_PAY_RATIO\"], axis=1, inplace=True)\n",
    "#Taiwan_pp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taiwan_df = Taiwan_df.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we apply a scalar to the whole dataset  to avoid one attribute from \n",
    "# skewing the results.\n",
    "#scaler = MinMaxScaler(copy=False)\n",
    "scaler = StandardScaler(copy=False)\n",
    "X = Taiwan_pp.drop([\"DEFAULT\",\"GENDER\"],axis=1) \n",
    "Y = Taiwan_pp[\"DEFAULT\"]\n",
    "G = Taiwan_pp[\"GENDER\"]\n",
    "#scaler = RobustScaler()\n",
    "Taiwan_scaled_df = scaler.fit_transform(Taiwan_pp)\n",
    "Taiwan_scaled_df = pd.DataFrame(Taiwan_scaled_df, columns=['LIMIT_BAL','GENDER','EDUCATION','MARRIAGE','AGE',\n",
    "                                            'PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6',\n",
    "                                            'AMT_PAY_RATIO1', 'AMT_PAY_RATIO2', 'AMT_PAY_RATIO3', \n",
    "                                            'AMT_PAY_RATIO4', 'AMT_PAY_RATIO5', 'AMT_PAY_RATIO6',\n",
    "                                            'AMT_PAY_RATIO', 'DEFAULT'])\n",
    "\n",
    "Taiwan_scaled_df = pd.concat([Taiwan_scaled_df,G,Y], axis=1) \n",
    "# Taiwan_scaled_df = pd.DataFrame(Taiwan_scaled_df, columns=['LIMIT_BAL','GENDER','EDUCATION','MARRIAGE','AGE',\n",
    "#                                                       'PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6',\n",
    "#                                                       'AMT_PAY_RATIO1', 'AMT_PAY_RATIO2', 'AMT_PAY_RATIO3', \n",
    "#                                                       'AMT_PAY_RATIO4', 'AMT_PAY_RATIO5', 'AMT_PAY_RATIO6',\n",
    "#                                                       'AMT_PAY_RATIO', 'DEFAULT'])\n",
    "# Taiwan_scaled_df = pd.DataFrame(Taiwan_scaled_df, columns=['LIMIT_BAL','GENDER','EDUCATION','MARRIAGE','AGE',\n",
    "#                                                       'AMT_PAY_RATIO1', 'AMT_PAY_RATIO2', 'AMT_PAY_RATIO3', \n",
    "#                                                       'AMT_PAY_RATIO4', 'AMT_PAY_RATIO5', 'AMT_PAY_RATIO6',\n",
    "#                                                       'AMT_PAY_RATIO', 'DEFAULT'])\n",
    "#Taiwan_scaled_df = pd.DataFrame(Taiwan_scaled_df, columns=['LIMIT_BAL','GENDER','EDUCATION','MARRIAGE','AGE',\n",
    "#                                                           'AMT_PAY_RATIO', 'DEFAULT'])\n",
    "#Taiwan_scaled_df = pd.DataFrame(Taiwan_scaled_df, columns=['GENDER','EDUCATION','MARRIAGE','AGE',\n",
    "#                                                           'DEFAULT'])\n",
    "#Taiwan_scaled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally write the cleaned dataset out to a csv\n",
    "Taiwan_scaled_df.to_csv('./input/Taiwan-Credit-Card-Cleaned.csv',index=False)\n",
    "# Taiwan_pp.drop([\"PAY_1\", \"PAY_2\" , \"PAY_3\" , \"PAY_4\" , \"PAY_5\" , \"PAY_6\" ], axis=1, inplace=True)\n",
    "# Taiwan_pp.to_csv('./input/Taiwan-Credit-Card-Cleaned.csv',index=False)\n",
    "# Taiwan_df.to_csv('./input/Taiwan-Credit-Card-Cleaned.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
