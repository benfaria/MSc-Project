{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data handling/display libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from typing import List, Union, Dict\n",
    "# put this lower down.....\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Import scikit-learn core slibraries\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import IBM's AI Fairness tooolbox\n",
    "from aif360.datasets import BinaryLabelDataset  # To handle the data\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric  # For calculating metrics\n",
    "from aif360.explainers import MetricTextExplainer  # For explaining metrics\n",
    "from aif360.algorithms.preprocessing import Reweighing  # Preprocessing technique\n",
    "\n",
    "# Warnings will be used to silence various model warnings for tidier output\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/Taiwan-Credit-Card-Cleaned.csv')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"DEFAULT\",axis=1)\n",
    "y = train[\"DEFAULT\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.7, random_state = 101, stratify=y)\n",
    "\n",
    "from collections import Counter\n",
    "print ('this is y_train Counter ()', Counter(y_train))\n",
    "print('this is y_test Counter ()', Counter(y_test))\n",
    "print('this is y_train unique', np.unique(y_train))\n",
    "print('this is y_test  unique', np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.GENDER.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make males 0, Females 1\n",
    "gender = {0 : -1.234323, 1: 0.810161} \n",
    "train.GENDER = [gender[item] for item in train.GENDER]\n",
    "train.GENDER.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasedlogmodel = LogisticRegression(random_state=101)\n",
    "biasedlogmodel.fit(X_train,y_train)\n",
    "logpredictions = biasedlogmodel.predict(X_test)\n",
    "print(f\"Logistic regression validation accuracy: {biasedlogmodel.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, logpredictions))\n",
    "confusion_matrix(y_test, logpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasedrfcmodel = RandomForestClassifier(n_estimators=100,max_depth=5)\n",
    "biasedrfcmodel.fit(X_train, y_train)\n",
    "rfcpredictions = biasedrfcmodel.predict(X_test)\n",
    "print(f\"Random forest validation accuracy: {biasedrfcmodel.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, rfcpredictions))\n",
    "confusion_matrix(y_test, rfcpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Binary Label Dataset to use with AIF360 APIs\n",
    "train_pp_bld = BinaryLabelDataset(df=pd.concat((X_train, y_train),\n",
    "                                               axis=1),\n",
    "                                  label_names=['DEFAULT'],\n",
    "                                  protected_attribute_names=['GENDER'],\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "privileged_groups = [{'GENDER': 0}]\n",
    "unprivileged_groups = [{'GENDER': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAdditions:\n",
    "    def explain(self,\n",
    "                disp: bool=True) -> Union[None, str]:\n",
    "        \"\"\"Explain everything available for the given metric.\"\"\"\n",
    "\n",
    "        # Find intersecting methods/attributes between MetricTextExplainer and provided metric.\n",
    "        inter = set(dir(self)).intersection(set(dir(self.metric)))\n",
    "\n",
    "        # Ignore private and dunder methods\n",
    "        metric_methods = [getattr(self, c) for c in inter if c.startswith('_') < 1]\n",
    "\n",
    "        # Call methods, join to new lines\n",
    "        s = \"\\n\".join([f() for f in metric_methods if callable(f)])\n",
    "\n",
    "        if disp:\n",
    "            print(s)\n",
    "        else:\n",
    "            return s  \n",
    "        \n",
    "class MetricTextExplainer_(MetricTextExplainer, MetricAdditions):\n",
    "    \"\"\"Combine explainer and .explain.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the metric object from the Binary Label Dataset.\n",
    "metric_train_bld = BinaryLabelDatasetMetric(train_pp_bld,\n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)\n",
    "print(\"# of instances  :\", metric_train_bld.num_instances())\n",
    "print(\"Base Rate       :\", metric_train_bld.base_rate())\n",
    "print(\"Consistency     :\", metric_train_bld.consistency())\n",
    "print(\"Disparate Impact:\", metric_train_bld.disparate_impact())\n",
    "print(\"Mean Difference :\", metric_train_bld.mean_difference())\n",
    "print(\"# of negatives(privileged)    :\", metric_train_bld.num_negatives(privileged=True))\n",
    "print(\"# of negatives(non-privileged):\", metric_train_bld.num_negatives(privileged=False))\n",
    "print(\"# of positives(privileged)    :\", metric_train_bld.num_positives(privileged=True))\n",
    "print(\"# of positives(non-privileged):\", metric_train_bld.num_positives(privileged=False))\n",
    "print(\"Statistical Parity Diference  :\", metric_train_bld.statistical_parity_difference()) \n",
    "print(\"\")\n",
    "print(\"This is the explainer\")\n",
    "# Create the explainer object\n",
    "explainer = MetricTextExplainer_(metric_train_bld)\n",
    "# Explain relevant metrics\n",
    "explainer.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now attempt to mitigate the bias using pre-processing Reweighing, apply to Training dataset\n",
    "rw = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "# trained model with new weights..\n",
    "train_pp_bld_f = rw.fit_transform(train_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe from Training dataset and new weights and have a look! \n",
    "pd.DataFrame({'GENDER': X_train.GENDER,\n",
    "              'DEFAULT': y_train,\n",
    "              'Original_weight': np.ones(shape=(X_train.shape[0],)),\n",
    "              'new_weight': train_pp_bld_f.instance_weights}).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new Dataframe \n",
    "unbiasedlogmodel = LogisticRegression()\n",
    "unbiasedlogmodel.fit(X_train, y_train, sample_weight=train_pp_bld_f.instance_weights)\n",
    "print(f\"Biased   Logistic regression validation accuracy: {biasedlogmodel.score(X_test,y_test)}\")\n",
    "print(f\"Unbiased Logistic regression validation accuracy: {unbiasedlogmodel.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiasedrfcmodel = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "unbiasedrfcmodel.fit(X_train, y_train, sample_weight=train_pp_bld_f.instance_weights)\n",
    "print(f\"Biased   Random forest validation accuracy: {biasedrfcmodel.score(X_test, y_test)}\")\n",
    "print(f\"Unbiased Random forest validation accuracy: {unbiasedrfcmodel.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(y_true: np.ndarray, preds: Dict[str, np.ndarray],\n",
    "             title: str='', \n",
    "             ax=None) -> None:\n",
    "    leg = []\n",
    "    for k, p in preds.items():\n",
    "        fpr, tpr, _ = roc_curve(y_true, p)\n",
    "        ax = sns.lineplot(x=fpr, \n",
    "                          y=tpr,\n",
    "                          ci=None,\n",
    "                          ax=ax\n",
    "                         )\n",
    "        leg.append(f\"{k}: {round(auc(fpr, tpr), 2)}\")\n",
    "   \n",
    "    ax.legend(leg)\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_ylabel('TPR')\n",
    "    sns.lineplot(x=[0, 1],\n",
    "                 y=[0, 1],\n",
    "                 color='gray',\n",
    "                 ax=ax)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    \n",
    "print('Accuracy:')\n",
    "display(pd.DataFrame({'LogReg': [biasedlogmodel.score(X_test, y_test), \n",
    "                                 unbiasedlogmodel.score(X_test, y_test)],\n",
    "                      'RFC': [biasedrfcmodel.score(X_test, y_test),\n",
    "                              unbiasedrfcmodel.score(X_test, y_test)]}, \n",
    "                     index =['Unfair', 'Fair']))\n",
    "\n",
    "print('AUC:')\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "plot_auc(y_test, \n",
    "         {'biased': biasedlogmodel.predict_proba(X_test)[:, 1],\n",
    "          'unbiased': unbiasedlogmodel.predict_proba(X_test)[:, 1]},\n",
    "         title='LR',\n",
    "         ax=ax[0]) \n",
    "plot_auc(y_test, \n",
    "         {'biased': biasedrfcmodel.predict_proba(X_test)[:, 1],\n",
    "          'unbiased': unbiasedrfcmodel.predict_proba(X_test)[:, 1]},\n",
    "         title='RFC',\n",
    "         ax=ax[1]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(mod: Union[LogisticRegression, RandomForestClassifier],\n",
    "                       names: List[str],\n",
    "                       scale=None) -> pd.DataFrame:\n",
    "    \"\"\"Return feature importance for LR or RFC models in a sorted DataFrame.\"\"\"\n",
    "    if type(mod) == LogisticRegression:\n",
    "        imp = np.abs(mod.coef_.squeeze()) / scale\n",
    "        var = np.zeros(shape=imp.shape)\n",
    "    elif type(mod) == RandomForestClassifier:\n",
    "        imps = np.array([fi.feature_importances_ for fi in mod.estimators_])\n",
    "        imp = imps.mean(axis=0)\n",
    "        var = imps.std(axis=0)\n",
    "\n",
    "    return pd.DataFrame({'feature': names,\n",
    "                         'importance': imp,\n",
    "                         'std': var}).sort_values('importance',\n",
    "                                                  ascending=False)\n",
    "\n",
    "def plot_feature_importance(**kwargs) -> None:\n",
    "    ax = sns.barplot(**kwargs)\n",
    "    for l in ax.get_xticklabels():\n",
    "        l.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(biasedlogmodel,\n",
    "                                                names=X_train.columns.tolist(),\n",
    "                                                scale=X_train.std()),\n",
    "                       ax=ax[1])\n",
    "_ = ax[0].set_title('LR Feature Importance - before Mitigating')\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(biasedrfcmodel,\n",
    "                                                names=X_train.columns.tolist()),\n",
    "                       ax=ax[0])\n",
    "_ = ax[1].set_title('RFC Feature Importance - before Mitigating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(unbiasedlogmodel,\n",
    "                                                names=X_train.columns.tolist(),\n",
    "                                                scale=X_train.std()),\n",
    "                       ax=ax[1])\n",
    "_ = ax[0].set_title('LR Feature Importance - after Mitigating')\n",
    "plot_feature_importance(x='feature', \n",
    "                        y='importance', \n",
    "                        data=feature_importance(unbiasedrfcmodel,\n",
    "                                                names=X_train.columns.tolist()),\n",
    "                       ax=ax[0])\n",
    "_ = ax[1].set_title('RFC Feature Importance - after Mitigating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(mod, x: pd.DataFrame, y_true: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate fairness metrics at each model threshold.\"\"\"\n",
    "    \n",
    "    # Create a BinaryLabelDataset (as before training)\n",
    "    bld = BinaryLabelDataset(df=pd.concat((x, y_true),\n",
    "                                               axis=1),\n",
    "                                  label_names=['DEFAULT'],\n",
    "                                  protected_attribute_names=['GENDER'],\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "    privileged_groups = [{'GENDER': 1}]\n",
    "    unprivileged_groups = [{'GENDER': 2}]\n",
    "    \n",
    "    # Create a second set to hold the predicted labels\n",
    "    bld_preds = bld.copy(deepcopy=True)\n",
    "    preds = mod.predict_proba(x)[:, 1] # for all the rows, retain the 2nd value.\n",
    "                                       # preds will contain a single column table of all \n",
    "                                       # probabilities that the classification will be 1.\n",
    "    accuracy = []\n",
    "    balanced_accuracy = []\n",
    "    disp_impact = []\n",
    "    average_abs_odds_difference = []\n",
    "    avg_odd_diff = []\n",
    "    equal_opportunity_difference = []\n",
    "    error_rate = []\n",
    "    \n",
    "    \n",
    "    # For threshold values between 0 and 1:\n",
    "    thresh = np.linspace(0.01, 0.99, 100) # generate 100 evenly spaced values from 0.01->0.99 \n",
    "    for t in thresh:\n",
    "        \n",
    "        # Apply threshold and set labels in bld for predictions\n",
    "        bld_preds.labels[preds > t] = 1  # labels is a structured dataset attribute.\n",
    "        bld_preds.labels[preds <= t] = 0\n",
    "\n",
    "        # Calculate the metrics for this threshold\n",
    "        valid_metric = ClassificationMetric(bld, bld_preds, \n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)\n",
    "\n",
    "        # Save the balanced accuracy of the model, and the metrics\n",
    "        accuracy.append(valid_metric.accuracy())\n",
    "        balanced_accuracy.append(0.5 * (valid_metric.true_positive_rate()\n",
    "                                        + valid_metric.true_negative_rate()))\n",
    "        disp_impact.append(np.abs(valid_metric.disparate_impact() - 0.5))\n",
    "        average_abs_odds_difference.append(valid_metric.average_abs_odds_difference())\n",
    "        avg_odd_diff.append(valid_metric.average_odds_difference())\n",
    "        equal_opportunity_difference.append(valid_metric.equal_opportunity_difference())\n",
    "        error_rate.append(valid_metric.error_rate())\n",
    "\n",
    "    # Return as df indexed by threshold\n",
    "    metrics = pd.DataFrame({'accuracy': accuracy,\n",
    "                            'balanced_accuracy': balanced_accuracy,\n",
    "                            'disparate_impact': disp_impact,\n",
    "                            'average_abs_odds_difference': average_abs_odds_difference,\n",
    "                            'avg_odds_diff': avg_odd_diff,\n",
    "                            'equal_opportunity_diff': equal_opportunity_difference},\n",
    "                            index=thresh)\n",
    "#'error_rate': error_rate},\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_metrics(metrics: pd.DataFrame, \n",
    "                 title: str='', **kwargs) -> None:\n",
    "    \"\"\"Plot the metrics df from calc_metrics with seaborn.\"\"\"\n",
    "    ax = sns.lineplot(data=metrics, \n",
    "                      **kwargs)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Classification threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for LR\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', RuntimeWarning)\n",
    "    \n",
    "    plot_metrics(calc_metrics(biasedlogmodel, X_test, y_test),\n",
    "                ax=ax[0],\n",
    "                title=\"LR: Biased\")\n",
    "    \n",
    "    plot_metrics(calc_metrics(unbiasedlogmodel, X_test, y_test),\n",
    "                ax=ax[1],\n",
    "                title='\"LR: Fair\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for RFC\n",
    "fig, ax = plt.subplots(nrows=1, \n",
    "                       ncols=2,\n",
    "                       figsize=(16, 6))\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', RuntimeWarning)\n",
    "    \n",
    "    plot_metrics(calc_metrics(biasedrfcmodel, X_test, y_test),\n",
    "                ax=ax[0],\n",
    "                title=\"RFC: Biased\")\n",
    "    \n",
    "    plot_metrics(calc_metrics(unbiasedrfcmodel, X_test, y_test),\n",
    "                ax=ax[1],\n",
    "                title='\"RFC: Fair\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
